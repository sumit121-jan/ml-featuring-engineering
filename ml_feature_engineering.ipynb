{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a parameter?\n"
      ],
      "metadata": {
        "id": "fOX6ftiUYr1v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A parameter in machine learning is an internal variable of a model that is learned from the data during training. Parameters determine how input data is transformed into the desired output. Examples include the weights in a linear regression or neural network. They are not set manually, but are adjusted automatically by the learning algorithm to minimize prediction errors"
      ],
      "metadata": {
        "id": "z77N-DoBZdZ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is correlation? What does negative correlation mean?\n",
        "\n"
      ],
      "metadata": {
        "id": "TMtCTYDPZeXD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlation is a statistical measure that describes the degree to which two variables move in relation to each other. It quantifies the strength and direction of a linear relationship between two variables. Correlation coefficients range from -1 (perfect negative correlation) to +1 (perfect positive correlation), with 0 indicating no linear relationship\n",
        "\n",
        "A negative correlation means that as one variable increases, the other decreases, and vice versa. The variables move in opposite directions. In statistics, a perfect negative correlation is represented by a coefficient of -1.0. For example, as the price of a product increases, the quantity demanded typically decreases, showing a negative correlation"
      ],
      "metadata": {
        "id": "9wGbpmX8Zg1q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Define Machine Learning. What are the main components in Machine Learning?\n"
      ],
      "metadata": {
        "id": "Qw4C1Y9sb_H9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Machine Learning (ML) is a branch of artificial intelligence (AI) that enables computers to learn from data, identify patterns, and make decisions with minimal human intervention. ML systems improve their performance automatically through experience"
      ],
      "metadata": {
        "id": "954CcmIgcCSY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main components in ML:\n",
        "\n",
        "Data: The raw information used to train models.\n",
        "\n",
        "Algorithms: The mathematical procedures or rules that process data and learn patterns.\n",
        "\n",
        "Models: The output of the learning process; models make predictions or decisions based on input data.\n",
        "\n",
        "Predictions: The results or outputs generated by the model when given new data"
      ],
      "metadata": {
        "id": "4CajCrV2cT59"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. How does loss value help in determining whether the model is good or not?\n"
      ],
      "metadata": {
        "id": "LTPG2IAvcmd_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The loss value quantifies the difference between the model's predictions and the actual target values. A lower loss indicates that the model's predictions are closer to the true values, signifying better performance. During training, the model updates its parameters to minimize the loss, thus improving accuracy. Monitoring the loss helps determine if the model is learning effectively or needs adjustments"
      ],
      "metadata": {
        "id": "cDl4jZBRcsod"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What are continuous and categorical variables?\n"
      ],
      "metadata": {
        "id": "9TYNyIEmczKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Continuous variables: Variables that can take any value within a range, including fractional values. Examples: height, weight, temperature.\n",
        "\n",
        "Categorical variables: Variables that represent qualitative groups or categories. They have a fixed number of possible values (categories), such as gender, color, or type of animal."
      ],
      "metadata": {
        "id": "Sf6X23qEc1kt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.How do we handle categorical variables in Machine Learning? What are the common techniques?"
      ],
      "metadata": {
        "id": "x58zqCEFc7bV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Categorical variables must be converted to numerical representations for most ML algorithms. Common techniques include:\n",
        "\n",
        "One-hot encoding: Creates binary columns for each category.\n",
        "\n",
        "Label encoding: Assigns a unique integer to each category.\n",
        "\n",
        "Target encoding: Replaces categories with the mean of the target variable for each category.\n",
        "\n",
        "Binary encoding, dummy encoding, effect encoding: Other advanced techniques for high-cardinality or specific modeling needs"
      ],
      "metadata": {
        "id": "u1P6ZLXJdFTg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What do you mean by training and testing a dataset?\n"
      ],
      "metadata": {
        "id": "29j32PaddMdg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training a dataset: Using a subset of data (training set) to teach the model to recognize patterns and relationships.\n",
        "\n",
        "Testing a dataset: Using a separate, unseen subset (test set) to evaluate how well the trained model generalizes to new data. This helps check for overfitting and ensures the model performs well on real-world data"
      ],
      "metadata": {
        "id": "l3y-MJYRdXTO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is sklearn.preprocessing?"
      ],
      "metadata": {
        "id": "hY3aeOr6dgWH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "sklearn.preprocessing is a module in the scikit-learn library that provides tools for preprocessing data, such as scaling, normalizing, encoding categorical variables, and transforming features. These transformations help prepare raw data for machine learning algorithms"
      ],
      "metadata": {
        "id": "_rCcqrfhdlPm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What is a Test set?"
      ],
      "metadata": {
        "id": "-mmS5hn5drih"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A test set is a portion of the dataset that is kept separate from the training data. It is used exclusively to evaluate the final model's performance and generalization ability. The model has never seen this data during training, ensuring an unbiased assessment."
      ],
      "metadata": {
        "id": "cKtxdGBvduZX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.How do we split data for model fitting (training and testing) in Python?\n",
        " How do you approach a Machine Learning problem?"
      ],
      "metadata": {
        "id": "SGXotuWMi48U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To split data for model fitting in Python, the most common approach is to use the train_test_split function from scikit-learn’s model_selection module. This function divides your dataset into two parts: a training set (used to train the model) and a test set (used to evaluate the model’s performance on unseen data).\n",
        "\n",
        "Typical usage:\n",
        "\n",
        "python\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        " X: features, y: target variable\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "test_size=0.2 means 20% of the data is reserved for testing, 80% for training.\n",
        "\n",
        "random_state ensures reproducibility.\n",
        "\n",
        "You can also specify train_size instead of test_size, or both.\n",
        "\n",
        "The function shuffles the data by default before splitting.\n",
        "\n",
        "A structured approach to a machine learning problem typically involves the following steps:\n",
        "\n",
        "Define the Problem\n",
        "\n",
        "Clearly state what you are trying to predict or classify. Understand the business or research objective.\n",
        "\n",
        "Collect and Explore Data\n",
        "\n",
        "Gather relevant and high-quality data. Perform exploratory data analysis (EDA) to understand data distribution, spot anomalies, and identify patterns.\n",
        "\n",
        "Prepare the Data\n",
        "\n",
        "Clean the data (handle missing values, outliers, duplicates), and preprocess features (scaling, encoding categorical variables, feature engineering).\n",
        "\n",
        "Split the Data\n",
        "\n",
        "Divide the data into training and test sets (and sometimes a validation set) to ensure unbiased model evaluation.\n",
        "\n",
        "Choose and Train a Model\n",
        "\n",
        "Select an appropriate algorithm based on the problem type (classification, regression, etc.) and train it on the training data.\n",
        "\n",
        "Evaluate the Model\n",
        "\n",
        "Assess model performance using suitable metrics (accuracy, precision, recall, RMSE, etc.) on the test set.\n",
        "\n",
        "Tune and Optimize\n",
        "\n",
        "Adjust model hyperparameters, try different algorithms, or engineer new features to improve performance.\n",
        "\n",
        "Deploy and Monitor\n",
        "\n",
        "Once satisfied, deploy the model for real-world use and monitor its performance over time."
      ],
      "metadata": {
        "id": "hf8Zk8V8jFTD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Why do we have to perform EDA before fitting a model to the data?\n"
      ],
      "metadata": {
        "id": "xjUr1-5UkHTS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "EDA helps us figure out what kind of data we have, how much missing data is there, and if there are any weird things going on... By doing EDA, we clean up our data and make sure it is ready for the next step. This way, we can be confident our results are accurate and based on good information. Basically, EDA helps us avoid building our analysis on a shaky foundation."
      ],
      "metadata": {
        "id": "0HCm0mmKkkr4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. What is correlation?\n"
      ],
      "metadata": {
        "id": "N1xQm_2eklyQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlation is a statistical measure that describes the degree to which two variables move in relation to each other. It quantifies the strength and direction of a linear relationship between two variables. Correlation coefficients range from -1 (perfect negative correlation) to +1 (perfect positive correlation), with 0 indicating no linear relationship"
      ],
      "metadata": {
        "id": "vt1cZTSWkuzN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What does negative correlation mean?\n"
      ],
      "metadata": {
        "id": "lMyp1dSkkv2l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A negative correlation means that as one variable increases, the other decreases, and vice versa. The variables move in opposite directions. In statistics, a perfect negative correlation is represented by a coefficient of -1.0. For example, as the price of a product increases, the quantity demanded typically decreases, showing a negative correlation"
      ],
      "metadata": {
        "id": "MaRtQ_MYkxuv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. How can you find correlation between variables in Python?\n"
      ],
      "metadata": {
        "id": "ozP9N3z_k4Jn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can find the correlation between variables in Python using several libraries, most commonly pandas, NumPy, and SciPy. Here are the main approaches:\n",
        "\n",
        "1. Using pandas\n",
        "If your data is in a pandas DataFrame, you can use the .corr() method to compute the correlation matrix for all numeric columns, or the correlation between two specific columns:\n",
        "\n",
        "python\n",
        "import pandas as pd\n",
        "\n",
        "For the whole DataFrame\n",
        "df.corr()\n",
        "\n",
        "For two specific columns\n",
        "df['col1'].corr(df['col2'])\n",
        "By default, .corr() computes the Pearson correlation. You can specify other methods like 'spearman' or 'kendall':\n",
        "\n",
        "python\n",
        "df.corr(method='spearman')\n",
        "df['col1'].corr(df['col2'], method='kendall')"
      ],
      "metadata": {
        "id": "k8hf1duHlDou"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. What is causation? Explain difference between correlation and causation with an example.\n"
      ],
      "metadata": {
        "id": "alPi-e8CoURe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is Causation?\n",
        "\n",
        "**Causation** means that a change in one variable directly causes a change in another variable. In other words, there is a cause-and-effect relationship: when variable A changes, it produces a change in variable B Establishing causation requires evidence that the effect is a result of the cause, not just that the two variables move together.\n",
        "\n",
        "---\n",
        "\n",
        "### Difference Between Correlation and Causation\n",
        "\n",
        "| Correlation                                                | Causation                                                         |\n",
        "|------------------------------------------------------------|-------------------------------------------------------------------|\n",
        "| Indicates a statistical association between variables | Indicates that one variable directly affects another |\n",
        "| Variables change together, but not necessarily due to cause | One variable’s change produces a change in the other              |\n",
        "| Does not prove cause-and-effect                             | Proves cause-and-effect                                           |\n",
        "| Can be due to coincidence or a third/confounding variable   | Requires evidence, often from experiments, to rule out other causes|\n",
        "| Example: Ice cream sales and sunburns rise together in summer, but one does not cause the other-both are influenced by hot weather | Example: Increasing the temperature of water causes it to boil    |\n",
        "\n",
        "---\n",
        "\n",
        "### **Example to Illustrate the Difference**\n",
        "\n",
        "- **Correlation Example:**  \n",
        "  There is a strong correlation between ice cream sales and the number of people who get sunburned. Both increase during the summer months. However, eating ice cream does not cause sunburns. Instead, a third variable-hot weather-causes both to increase. This is correlation, not causation\n",
        "\n",
        "- **Causation Example:**  \n",
        "  If research shows that taking a specific medication (variable A) directly leads to a reduction in blood pressure (variable B), and this has been demonstrated through controlled experiments that rule out other factors, then we have established causation\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Takeaways**\n",
        "\n",
        "- **Correlation** is about variables moving together, but not necessarily because one causes the other.\n",
        "- **Causation** is when one variable’s change produces a direct effect on another.\n",
        "- Correlation does **not** imply causation-mistaking the two can lead to false conclusions and poor decision-making.\n",
        "\n",
        "---\n",
        "\n",
        "**In summary:**  \n",
        "Causation is a cause-and-effect relationship between two variables, while correlation simply means they move together. Establishing causation requires more rigorous evidence, often through experiments, to rule out other explanations for the observed relationship"
      ],
      "metadata": {
        "id": "OI3xCb8zpulD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. What is an Optimizer? What are different types of optimizers? Explain each with an example.\n"
      ],
      "metadata": {
        "id": "_HNt8EHhqiMl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An optimizer is an algorithm used in machine learning and deep learning to adjust the parameters (weights and biases) of a model during training in order to minimize the loss function. The optimizer determines how the model learns from data by updating parameters based on the gradients calculated from the loss, thereby improving the model’s predictions.\n",
        "\n",
        "\"Imagine you’re on a mountain and your goal is to get to the bottom where there’s a beautiful valley (the optimal solution). The optimizer takes steps in the direction where the ground declines the most, adjusting parameters to minimize the loss function.\"\n",
        "\n",
        "Types of Optimizers and Their Explanations\n",
        "Below are some of the most widely used optimizers in machine learning and deep learning, with brief explanations and examples for each:\n",
        "\n",
        "1. Gradient Descent (GD)\n",
        "How it works: Updates parameters by computing the gradient of the loss function using the entire dataset and moves in the direction of the steepest descent (negative gradient).\n",
        "\n",
        "Pros: Simple and effective for small datasets.\n",
        "\n",
        "Cons: Slow for large datasets, can get stuck in local minima, sensitive to learning rate.\n",
        "\n",
        "Example: Used in linear regression to find the best-fitting line by minimizing mean squared error.\n",
        "\n",
        "2. Stochastic Gradient Descent (SGD)\n",
        "How it works: Similar to GD, but updates parameters using a single data point or a small batch (mini-batch) at each iteration, introducing randomness.\n",
        "\n",
        "Pros: Faster and more scalable for large datasets, can escape local minima.\n",
        "\n",
        "Cons: Updates are noisy, may require more epochs and careful tuning of learning rate.\n",
        "\n",
        "Example: Commonly used in training neural networks for image classification.\n",
        "\n",
        "3. Momentum\n",
        "How it works: Builds on SGD by adding a fraction of the previous update to the current update, helping to accelerate convergence and dampen oscillations.\n",
        "\n",
        "Pros: Faster convergence, helps overcome local minima.\n",
        "\n",
        "Cons: Sensitive to hyperparameters (momentum factor).\n",
        "\n",
        "Example: Used in deep neural networks to improve training speed and stability.\n",
        "\n",
        "4. Nesterov Accelerated Gradient (NAG)\n",
        "How it works: An improvement on momentum; computes the gradient not just at the current position but anticipates the future position, leading to more accurate updates.\n",
        "\n",
        "Pros: Faster and more precise convergence than vanilla momentum.\n",
        "\n",
        "Cons: More computationally expensive.\n",
        "\n",
        "Example: Used in deep learning tasks requiring faster convergence.\n",
        "\n",
        "5. Adagrad\n",
        "How it works: Adapts the learning rate for each parameter based on the sum of past squared gradients, making larger updates for infrequent parameters.\n",
        "\n",
        "Pros: Good for sparse data (e.g., NLP).\n",
        "\n",
        "Cons: Learning rate can become excessively small over time.\n",
        "\n",
        "Example: Used in natural language processing tasks.\n",
        "\n",
        "6. RMSProp\n",
        "How it works: Modifies Adagrad by using a moving average of squared gradients to normalize the learning rate, preventing it from shrinking too much.\n",
        "\n",
        "Pros: Works well for non-stationary objectives, effective for RNNs and deep networks.\n",
        "\n",
        "Cons: Requires tuning of decay rate hyperparameter.\n",
        "\n",
        "Example: Used in training recurrent neural networks.\n",
        "\n",
        "7. Adam (Adaptive Moment Estimation)\n",
        "How it works: Combines the benefits of momentum and RMSProp by maintaining moving averages of both the gradients and their squares, with bias correction.\n",
        "\n",
        "Pros: Fast convergence, less sensitive to hyperparameters, widely used and robust.\n",
        "\n",
        "Cons: Can sometimes generalize worse than SGD in some scenarios.\n",
        "\n",
        "Example: The default optimizer for many deep learning frameworks; used in image recognition, NLP, and more"
      ],
      "metadata": {
        "id": "2eqBzWsSrHcO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. What is sklearn.linear_model ?"
      ],
      "metadata": {
        "id": "EAwUDaD_rcQw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "sklearn.linear_model is a module in the scikit-learn library that provides a variety of linear models for regression and classification tasks. These models assume that the target variable is a linear combination of the input features, making them both simple and interpretable"
      ],
      "metadata": {
        "id": "Up91XTFerfi0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What does model.fit() do? What arguments must be given?"
      ],
      "metadata": {
        "id": "egg5oD9Srojj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "model.fit() trains (fits) the machine learning model to your data.\n",
        "\n",
        "Required arguments:\n",
        "\n",
        "X: Feature matrix (input variables, usually 2D array or DataFrame)\n",
        "\n",
        "y: Target variable (output values, usually 1D array or Series)\n",
        "\n",
        "Example:\n",
        "\n",
        "python\n",
        "model.fit(X_train, y_train)\n",
        "This finds the best parameters for the model using the training data."
      ],
      "metadata": {
        "id": "8OYN220Ur0qd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. What does model.predict() do? What arguments must be given?\n"
      ],
      "metadata": {
        "id": "Z58E0LhBr48-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "model.predict() uses the trained model to make predictions on new data.\n",
        "\n",
        "Required argument:\n",
        "\n",
        "X: Feature matrix of new/unseen data (same structure as training features)\n",
        "\n",
        "Example:\n",
        "\n",
        "python\n",
        "predictions = model.predict(X_test)\n",
        "This returns predicted values for each row in X_test."
      ],
      "metadata": {
        "id": "ljXm-HaOr8cJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. What are continuous and categorical variables?\n"
      ],
      "metadata": {
        "id": "vLOdBWyfsANp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Continuous variables are quantitative variables that can take any value within a given range, including fractional or decimal values. They are measurable and can assume an infinite number of possible values within that range. Examples include height, weight, temperature, and delivery time-any variable that can be measured on a scale and can have any value, not just whole numbers.\n",
        "\n",
        "Categorical variables (also called qualitative variables) represent distinct groups or categories. They contain a finite number of possible values, usually describing qualities or characteristics rather than measurements. Categorical variables can be:\n",
        "\n",
        "Nominal: Categories with no logical order (e.g., hair color, city, pizza topping).\n",
        "\n",
        "Ordinal: Categories with a logical order (e.g., education level, pizza size: small, medium, large)"
      ],
      "metadata": {
        "id": "ttXcRtRmsHyz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. What is feature scaling? How does it help in Machine Learning?\n"
      ],
      "metadata": {
        "id": "VyZ8Gq43sTfY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature scaling is the process of transforming numerical features to a common scale or range, such as 0–1 (normalization) or mean 0 and standard deviation 1 (standardization).\n",
        "\n",
        "Why it helps:\n",
        "\n",
        "Prevents features with large ranges from dominating the model.\n",
        "\n",
        "Improves convergence speed and accuracy for many algorithms, especially those using distance calculations (e.g., k-NN, SVM, K-Means) or gradient descent.\n",
        "\n",
        "Ensures all features contribute equally and avoids bias.\n",
        "\n",
        "Makes model results more interpretable"
      ],
      "metadata": {
        "id": "Rr3m2ClLsLpv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. How do we perform scaling in Python?\n"
      ],
      "metadata": {
        "id": "qGyxwCyUsWww"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use scikit-learn’s sklearn.preprocessing module.\n",
        "\n",
        "Example (Standardization):\n",
        "\n",
        "python\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "Example (Normalization):\n",
        "\n",
        "python\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "Always fit the scaler on training data and transform both training and test data using the fitted scaler."
      ],
      "metadata": {
        "id": "eHDcWlLnsbSm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. What is sklearn.preprocessing?\n"
      ],
      "metadata": {
        "id": "ow8x9oqesjZN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "sklearn.preprocessing is a module in scikit-learn that provides tools for preprocessing data, such as scaling (standardization, normalization), encoding categorical variables, and transforming features to prepare them for machine learning algorithms"
      ],
      "metadata": {
        "id": "JilU9HEztBO4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. How do we split data for model fitting (training and testing) in Python?\n"
      ],
      "metadata": {
        "id": "Ror64XHPtGg9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To split data for model fitting in Python, the most common approach is to use the train_test_split function from scikit-learn’s model_selection module. This function divides your dataset into two parts: a training set (used to train the model) and a test set (used to evaluate the model’s performance on unseen data).\n",
        "\n",
        "Typical usage:\n",
        "\n",
        "python\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        " X: features, y: target variable\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "test_size=0.2 means 20% of the data is reserved for testing, 80% for training.\n",
        "\n",
        "random_state ensures reproducibility.\n",
        "\n",
        "You can also specify train_size instead of test_size, or both.\n",
        "\n",
        "The function shuffles the data by default before splitting."
      ],
      "metadata": {
        "id": "-9aUL12ltKLk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Explain data encoding?\n"
      ],
      "metadata": {
        "id": "sNvHbb01ta1k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data encoding is the process of converting categorical variables into a numerical format so that machine learning algorithms can use them.\n",
        "\n",
        "Common techniques:\n",
        "\n",
        "Label Encoding: Assigns each category a unique integer.\n",
        "\n",
        "One-Hot Encoding: Creates binary columns for each category.\n",
        "\n",
        "Ordinal Encoding: Assigns ordered integers to ordered categories.\n",
        "\n",
        "Encoding is often performed using sklearn.preprocessing tools such as LabelEncoder and OneHotEncoder."
      ],
      "metadata": {
        "id": "wwkJ5tYwtfYu"
      }
    }
  ]
}